import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# === –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
MODEL_PATH = "./saiga-mistral-lora-output"
BASE_MODEL = "IlyaGusev/saiga_mistral_7b_merged"

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ —Å LoRA ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map="auto")
model = PeftModel.from_pretrained(model, MODEL_PATH)
model.eval()

# === –û—Ç–∑—ã–≤—ã –¥–ª—è —Ç–µ—Å—Ç–∞ ===
reviews = [
    "–ö—É—Ä—Ç–∫–∞ —à–∏–∫–∞—Ä–Ω–∞—è! –£–¥–æ–±–Ω–∞—è, –ª—ë–≥–∫–∞—è –∏ —Ç–µ–ø–ª–∞—è.",
    "–ü–∞–ª—å—Ç–æ –ø—Ä–∏—à–ª–æ –Ω–µ —Ç–æ–≥–æ —Ü–≤–µ—Ç–∞, –Ω–æ —Å–∏–¥–∏—Ç –æ—Ç–ª–∏—á–Ω–æ.",
    "–•–æ—Ä–æ—à–∞—è –≤–µ—Ç—Ä–æ–≤–∫–∞ –∑–∞ —Å–≤–æ—é —Ü–µ–Ω—É. –ú—É–∂ –¥–æ–≤–æ–ª–µ–Ω.",
]

# === –®–∞–±–ª–æ–Ω prompt'–∞ (—Ç–æ—Ç –∂–µ, —á—Ç–æ –≤ –æ–±—É—á–µ–Ω–∏–∏) ===
def build_prompt(review_text):
    return (
        "<|system|>\n–í—ã ‚Äî –≤–µ–∂–ª–∏–≤—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞. "
        "–ù–∞–ø–∏—à–∏—Ç–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Ç–∑—ã–≤ –∫–ª–∏–µ–Ω—Ç–∞.\n"
        f"<|user|>\n–û—Ç–∑—ã–≤: {review_text}\n<|assistant|>"
    )

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
for i, review in enumerate(reviews, 1):
    prompt = build_prompt(review)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    print(f"\nüîπ –û—Ç–∑—ã–≤ {i}:\nüìù {review}\nüì£ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n{response}\n" + "-" * 60)

