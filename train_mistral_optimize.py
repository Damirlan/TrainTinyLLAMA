import os
import gc
import pandas as pd
import torch

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    Trainer,
    BitsAndBytesConfig,
)
from peft import get_peft_model, LoraConfig, TaskType

# üîÅ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
gc.collect()
torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "IlyaGusev/saiga_mistral_7b_merged"
CSV_PATH = "reviews_with_replies.csv"
OUTPUT_DIR = "./saiga-mistral-reply-lora"
MAX_LENGTH = 512

# üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv(CSV_PATH)

# üßæ –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥
def format_row(row):
    prompt = (
        f"<s>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:\n"
        f"–†–µ–π—Ç–∏–Ω–≥: {row['rating']}\n"
        f"–¢–æ–≤–∞—Ä: {row['product_name']}\n"
        f"–û—Ç–∑—ã–≤: {row['review_text']}\n"
        f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
    )
    return {"text": f"{prompt} {row['reply']}</s>"}

formatted = [format_row(row) for _, row in df.iterrows()]
dataset = Dataset.from_list(formatted)

# üî† –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

def tokenize(example):
    return tokenizer(
        example["text"],
        max_length=MAX_LENGTH,
        padding="max_length",
        truncation=True
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# üß† LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# üß† BitsAndBytes 4-bit config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    quantization_config=bnb_config
)
model = get_peft_model(model, lora_config)
model.gradient_checkpointing_enable()

# üìö –ö–æ–ª–ª–∞—Ç–æ—Ä
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ‚öôÔ∏è –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=2,
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    bf16=False,
    report_to="none"
)

# üöÄ Trainer
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

# üß† –û–±—É—á–µ–Ω–∏–µ
trainer.train()

# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

