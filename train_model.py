import os
import pandas as pd
import torch
from transformers import (
    LlamaForCausalLM,
    LlamaTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import Dataset

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è CUDA-–ø–∞–º—è—Ç—å—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("–ó–∞–≥—Ä—É–∑–∫–∞ CSV...")
df = pd.read_csv("reviews_with_replies.csv", encoding="utf-8")
print(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")

# 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤
def format_example(example):
    prompt = (
        f"–†–µ–π—Ç–∏–Ω–≥: {example['rating']}\n"
        f"–¢–æ–≤–∞—Ä: {example['product_name']}\n"
        f"–û—Ç–∑—ã–≤: {example['review_text']}\n"
        "–û—Ç–≤–µ—Ç:"
    )
    completion = example['reply']
    return {"text": prompt + " " + completion}

print("–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã...")
texts = df.apply(format_example, axis=1).tolist()
texts_for_dataset = [x["text"] for x in texts]
print(f"–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n{texts_for_dataset[0][:300]}...")

dataset = Dataset.from_dict({"text": texts_for_dataset})

# 3. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ dtype
use_gpu = torch.cuda.is_available()
device_map = "auto" if use_gpu else None
torch_dtype = torch.float16 if use_gpu else torch.float32
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {'GPU' if use_gpu else 'CPU'}")

# 4. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å...")
tokenizer = LlamaTokenizer.from_pretrained(model_name)

model = LlamaForCausalLM.from_pretrained(
    model_name,
    device_map=device_map,
    torch_dtype=torch_dtype,
    offload_folder="offload" if use_gpu else None,
    offload_state_dict=use_gpu,
)

# –í–∫–ª—é—á–∞–µ–º gradient checkpointing (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
model.gradient_checkpointing_enable()

# 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
print(f"–ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n{tokenized_datasets[0]}")

# 6. Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# 7. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir="./tinyllama-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    bf16=True,  # ‚úÖ A5000 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bfloat16
    report_to="none",
    load_best_model_at_end=False,
    logging_dir="./logs",
)
# 8. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    data_collator=data_collator,
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
trainer.train()
print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

# 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
trainer.save_model("./tinyllama-finetuned")
tokenizer.save_pretrained("./tinyllama-finetuned")
print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

