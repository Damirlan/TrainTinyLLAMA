import os
import pandas as pd
import torch
from transformers import (
    LlamaForCausalLM,
    LlamaTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import Dataset

# –ß—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–∞–º—è—Ç–∏ CUDA (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
print("–ó–∞–≥—Ä—É–∑–∫–∞ CSV...")
df = pd.read_csv("reviews_with_replies.csv", encoding='utf-8')
print(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")

# 2. –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã
def format_example(example):
    prompt = (f"–†–µ–π—Ç–∏–Ω–≥: {example['rating']}\n"
              f"–¢–æ–≤–∞—Ä: {example['product_name']}\n"
              f"–û—Ç–∑—ã–≤: {example['review_text']}\n"
              "–û—Ç–≤–µ—Ç:")
    completion = example['reply']
    return {"text": prompt + " " + completion}

print("–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã...")
texts = df.apply(format_example, axis=1).tolist()
texts_for_dataset = [x["text"] for x in texts]
print(f"–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n{texts_for_dataset[0][:300]}...")

dataset = Dataset.from_dict({"text": texts_for_dataset})

# 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å...")
tokenizer = LlamaTokenizer.from_pretrained(model_name)

model = LlamaForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    offload_folder="offload",       # offload —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫
    offload_state_dict=True,        # –≤—ã–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    torch_dtype=torch.float16       # 16-–±–∏—Ç–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (fp16)
)

# –í–∫–ª—é—á–∞–µ–º gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
model.gradient_checkpointing_enable()

# 4. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

print(f"–ü—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n{tokenized_datasets[0]}")

# 5. Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = TrainingArguments(
    output_dir="./tinyllama-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,  # –£–º–µ–Ω—å—à–∞–µ–º batch size –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞–º—è—Ç–∏
    save_steps=500,
    save_total_limit=2,
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    fp16=False,                    # GTX 1660 –Ω–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å fp16 —á–µ—Ä–µ–∑ AMP, –ø–æ—ç—Ç–æ–º—É False
    report_to="none",
    load_best_model_at_end=False,
)

# 7. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    data_collator=data_collator,
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU...")
trainer.train()
print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

# 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
print("–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
trainer.save_model("./tinyllama-finetuned")
tokenizer.save_pretrained("./tinyllama-finetuned")
print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
