import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

MODEL_NAME = "IlyaGusev/saiga_mistral_7b_merged"

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)
model.eval()

gen_conf = GenerationConfig.from_pretrained(MODEL_NAME)

# –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
examples = [
    {
        "rating": 5,
        "product_name": "—Ç—ë–ø–ª—ã–π —Å–≤–∏—Ç–µ—Ä",
        "review_text": "–û—á–µ–Ω—å –º—è–≥–∫–∏–π –∏ –ø—Ä–∏—è—Ç–Ω—ã–π, –∏–¥–µ–∞–ª—å–Ω–æ –Ω–∞ –∑–∏–º—É!"
    },
    {
        "rating": 2,
        "product_name": "–ø—ã–ª–µ—Å–æ—Å",
        "review_text": "–°–ª–∞–±–æ–µ –≤—Å–∞—Å—ã–≤–∞–Ω–∏–µ, –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π —à—É–º."
    },
]

def generate_reply(example):
    prompt = (
        f"–¢—ã ‚Äî –≤–µ–∂–ª–∏–≤—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞.\n"
        f"–†–µ–π—Ç–∏–Ω–≥: {example['rating']}\n"
        f"–¢–æ–≤–∞—Ä: {example['product_name']}\n"
        f"–û—Ç–∑—ã–≤: {example['review_text']}\n"
        f"–û—Ç–≤–µ—Ç:"
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            generation_config=gen_conf,
            max_new_tokens=100,
            eos_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(output[0], skip_special_tokens=True)
    reply = text[len(prompt):].split("–†–µ–π—Ç–∏–Ω–≥:")[0].strip()
    return reply

print("\nüìä –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ Saiga-Mistral:\n" + "-"*50)
for i, ex in enumerate(examples, 1):
    reply = generate_reply(ex)
    print(f"[–ü—Ä–∏–º–µ—Ä {i}]")
    print(f"–¢–æ–≤–∞—Ä: {ex['product_name']}, –†–µ–π—Ç–∏–Ω–≥: {ex['rating']}")
    print(f"–û—Ç–∑—ã–≤: {ex['review_text']}")
    print(f"–û—Ç–≤–µ—Ç: {reply}\n" + "-"*50)

